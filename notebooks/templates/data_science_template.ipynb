{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bee1d14",
   "metadata": {},
   "source": [
    "# Data Science Workflow Template\n",
    "\n",
    "**Purpose**: Template notebook for data scientists working with the Risk Platform\n",
    "\n",
    "**Use Cases**:\n",
    "- Model development and backtesting\n",
    "- Advanced analytics and research\n",
    "- Custom risk metric development\n",
    "\n",
    "**Author**: Risk Platform Team  \n",
    "**Created**: September 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53ac8f2",
   "metadata": {},
   "source": [
    "## 🔧 Development Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae70aa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Development imports - add your preferred libraries here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Risk platform specific imports\n",
    "import sys\n",
    "sys.path.append('/home/jovyan/shared/libs')  # Risk platform libraries\n",
    "\n",
    "# Configuration\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ Development environment configured\")\n",
    "print(f\"📊 Pandas version: {pd.__version__}\")\n",
    "print(f\"🔢 NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9314ee35",
   "metadata": {},
   "source": [
    "## 🔌 Risk Platform Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ae0d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiskPlatformClient:\n",
    "    \"\"\"Client for Risk Platform API integration\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url=\"http://fastapi-service.default.svc.cluster.local\"):\n",
    "        self.base_url = base_url\n",
    "        self.headers = {\n",
    "            'Content-Type': 'application/json',\n",
    "            'Accept': 'application/json'\n",
    "        }\n",
    "    \n",
    "    def get_portfolios(self):\n",
    "        \"\"\"Retrieve all portfolios\"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{self.base_url}/api/v1/portfolios\", headers=self.headers)\n",
    "            return response.json() if response.status_code == 200 else []\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    def calculate_risk(self, portfolio_data):\n",
    "        \"\"\"Calculate risk metrics for portfolio\"\"\"\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/api/v1/risk/calculate\", \n",
    "                headers=self.headers, \n",
    "                json=portfolio_data\n",
    "            )\n",
    "            return response.json() if response.status_code == 200 else {}\n",
    "        except:\n",
    "            return {}\n",
    "    \n",
    "    def get_market_data(self, symbols, start_date, end_date):\n",
    "        \"\"\"Retrieve market data\"\"\"\n",
    "        try:\n",
    "            data = {\n",
    "                \"symbols\": symbols,\n",
    "                \"start_date\": start_date,\n",
    "                \"end_date\": end_date\n",
    "            }\n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/api/v1/market/data\", \n",
    "                headers=self.headers, \n",
    "                json=data\n",
    "            )\n",
    "            return response.json() if response.status_code == 200 else {}\n",
    "        except:\n",
    "            return {}\n",
    "\n",
    "# Initialize client\n",
    "risk_client = RiskPlatformClient()\n",
    "print(\"✅ Risk Platform client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a58ac29",
   "metadata": {},
   "source": [
    "## 📊 Data Exploration Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4853d364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_dataset(df, title=\"Dataset\"):\n",
    "    \"\"\"Comprehensive dataset exploration function\"\"\"\n",
    "    print(f\"📊 {title} Exploration\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    print(\"\\nColumn Information:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    print(\"\\nNumerical Summary:\")\n",
    "    display(df.describe())\n",
    "    \n",
    "    print(\"\\nMissing Values:\")\n",
    "    missing = df.isnull().sum()\n",
    "    if missing.sum() > 0:\n",
    "        display(missing[missing > 0].sort_values(ascending=False))\n",
    "    else:\n",
    "        print(\"No missing values found\")\n",
    "    \n",
    "    # Correlation matrix for numerical columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 1:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        correlation_matrix = df[numeric_cols].corr()\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "        plt.title(f'{title} - Correlation Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Example usage - replace with your actual dataset\n",
    "# sample_data = pd.DataFrame({\n",
    "#     'returns': np.random.normal(0.001, 0.02, 1000),\n",
    "#     'volume': np.random.lognormal(15, 1, 1000),\n",
    "#     'volatility': np.random.gamma(2, 0.01, 1000)\n",
    "# })\n",
    "# explore_dataset(sample_data, \"Sample Financial Data\")\n",
    "\n",
    "print(\"📊 Data exploration template ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482ab144",
   "metadata": {},
   "source": [
    "## 🤖 Model Development Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21e294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiskModelFramework:\n",
    "    \"\"\"Framework for developing and testing risk models\"\"\"\n",
    "    \n",
    "    def __init__(self, name=\"Risk Model\"):\n",
    "        self.name = name\n",
    "        self.model = None\n",
    "        self.training_data = None\n",
    "        self.test_data = None\n",
    "        self.predictions = None\n",
    "        self.metrics = {}\n",
    "    \n",
    "    def prepare_data(self, data, target_col, test_size=0.2, random_state=42):\n",
    "        \"\"\"Prepare data for model training\"\"\"\n",
    "        X = data.drop(columns=[target_col])\n",
    "        y = data[target_col]\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "        \n",
    "        self.training_data = (X_train, y_train)\n",
    "        self.test_data = (X_test, y_test)\n",
    "        \n",
    "        print(f\"✅ Data prepared: Training {X_train.shape}, Test {X_test.shape}\")\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def train_model(self, model, X_train=None, y_train=None):\n",
    "        \"\"\"Train the risk model\"\"\"\n",
    "        if X_train is None or y_train is None:\n",
    "            X_train, y_train = self.training_data\n",
    "        \n",
    "        self.model = model\n",
    "        self.model.fit(X_train, y_train)\n",
    "        \n",
    "        print(f\"✅ Model {self.name} trained successfully\")\n",
    "        return self.model\n",
    "    \n",
    "    def evaluate_model(self, X_test=None, y_test=None):\n",
    "        \"\"\"Evaluate model performance\"\"\"\n",
    "        if X_test is None or y_test is None:\n",
    "            X_test, y_test = self.test_data\n",
    "        \n",
    "        self.predictions = self.model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        self.metrics = {\n",
    "            'mse': mean_squared_error(y_test, self.predictions),\n",
    "            'rmse': np.sqrt(mean_squared_error(y_test, self.predictions)),\n",
    "            'r2': r2_score(y_test, self.predictions),\n",
    "            'mae': np.mean(np.abs(y_test - self.predictions))\n",
    "        }\n",
    "        \n",
    "        print(f\"📊 {self.name} Performance Metrics:\")\n",
    "        for metric, value in self.metrics.items():\n",
    "            print(f\"   {metric.upper()}: {value:.4f}\")\n",
    "        \n",
    "        return self.metrics\n",
    "    \n",
    "    def plot_results(self, y_test=None):\n",
    "        \"\"\"Plot model results\"\"\"\n",
    "        if y_test is None:\n",
    "            _, y_test = self.test_data\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Actual vs Predicted\n",
    "        ax1.scatter(y_test, self.predictions, alpha=0.6)\n",
    "        ax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "        ax1.set_xlabel('Actual Values')\n",
    "        ax1.set_ylabel('Predicted Values')\n",
    "        ax1.set_title(f'{self.name} - Actual vs Predicted')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Residuals\n",
    "        residuals = y_test - self.predictions\n",
    "        ax2.scatter(self.predictions, residuals, alpha=0.6)\n",
    "        ax2.axhline(y=0, color='r', linestyle='--')\n",
    "        ax2.set_xlabel('Predicted Values')\n",
    "        ax2.set_ylabel('Residuals')\n",
    "        ax2.set_title(f'{self.name} - Residuals Plot')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"📈 {self.name} results plotted\")\n",
    "\n",
    "print(\"🤖 Risk model framework ready\")\n",
    "print(\"📝 Usage: framework = RiskModelFramework('My Risk Model')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1497577f",
   "metadata": {},
   "source": [
    "## 📈 Example: Volatility Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fa8b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample dataset for volatility prediction\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Generate synthetic features\n",
    "sample_data = pd.DataFrame({\n",
    "    'previous_volatility': np.random.gamma(2, 0.01, n_samples),\n",
    "    'volume': np.random.lognormal(15, 1, n_samples),\n",
    "    'price_change': np.random.normal(0, 0.02, n_samples),\n",
    "    'market_cap': np.random.lognormal(20, 2, n_samples),\n",
    "    'sector_beta': np.random.normal(1, 0.3, n_samples)\n",
    "})\n",
    "\n",
    "# Create target variable (future volatility)\n",
    "sample_data['future_volatility'] = (\n",
    "    0.7 * sample_data['previous_volatility'] + \n",
    "    0.2 * np.abs(sample_data['price_change']) + \n",
    "    0.1 * sample_data['sector_beta'] * 0.01 +\n",
    "    np.random.normal(0, 0.005, n_samples)\n",
    ")\n",
    "\n",
    "print(\"📊 Sample dataset created for volatility prediction\")\n",
    "print(f\"Dataset shape: {sample_data.shape}\")\n",
    "display(sample_data.head())\n",
    "\n",
    "# Initialize and use the framework\n",
    "vol_model = RiskModelFramework(\"Volatility Prediction Model\")\n",
    "\n",
    "# Prepare data\n",
    "X_train, X_test, y_train, y_test = vol_model.prepare_data(\n",
    "    sample_data, 'future_volatility', test_size=0.2\n",
    ")\n",
    "\n",
    "# Train Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "vol_model.train_model(rf_model, X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "metrics = vol_model.evaluate_model(X_test, y_test)\n",
    "\n",
    "# Plot results\n",
    "vol_model.plot_results(y_test)\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=feature_importance, x='importance', y='feature', palette='viridis')\n",
    "plt.title('Feature Importance - Volatility Prediction Model')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🎯 Feature Importance:\")\n",
    "display(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de47089",
   "metadata": {},
   "source": [
    "## 💾 Save & Export Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1fea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model persistence utilities\n",
    "import pickle\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "def save_model(model, name, save_dir=\"/home/jovyan/shared/models\"):\n",
    "    \"\"\"Save model with metadata\"\"\"\n",
    "    save_path = Path(save_dir)\n",
    "    save_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Save model\n",
    "    model_file = save_path / f\"{name}_model.joblib\"\n",
    "    joblib.dump(model, model_file)\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'name': name,\n",
    "        'created_date': datetime.now().isoformat(),\n",
    "        'model_type': type(model).__name__,\n",
    "        'file_path': str(model_file)\n",
    "    }\n",
    "    \n",
    "    metadata_file = save_path / f\"{name}_metadata.json\"\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"✅ Model saved: {model_file}\")\n",
    "    print(f\"📋 Metadata saved: {metadata_file}\")\n",
    "    return model_file, metadata_file\n",
    "\n",
    "def load_model(name, save_dir=\"/home/jovyan/shared/models\"):\n",
    "    \"\"\"Load model with metadata\"\"\"\n",
    "    save_path = Path(save_dir)\n",
    "    model_file = save_path / f\"{name}_model.joblib\"\n",
    "    metadata_file = save_path / f\"{name}_metadata.json\"\n",
    "    \n",
    "    if model_file.exists():\n",
    "        model = joblib.load(model_file)\n",
    "        \n",
    "        if metadata_file.exists():\n",
    "            with open(metadata_file, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "            print(f\"✅ Model loaded: {name}\")\n",
    "            print(f\"📅 Created: {metadata.get('created_date', 'Unknown')}\")\n",
    "            print(f\"🔧 Type: {metadata.get('model_type', 'Unknown')}\")\n",
    "        else:\n",
    "            print(f\"✅ Model loaded: {name} (no metadata)\")\n",
    "        \n",
    "        return model\n",
    "    else:\n",
    "        print(f\"❌ Model not found: {name}\")\n",
    "        return None\n",
    "\n",
    "# Example: Save the volatility model\n",
    "# save_model(vol_model.model, \"volatility_prediction_v1\")\n",
    "\n",
    "print(\"💾 Model persistence utilities ready\")\n",
    "print(\"📝 Usage: save_model(your_model, 'model_name')\")\n",
    "print(\"📝 Usage: loaded_model = load_model('model_name')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a42c8c",
   "metadata": {},
   "source": [
    "## 📋 Experiment Tracking Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a4f345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment tracking utilities\n",
    "experiment_log = []\n",
    "\n",
    "def log_experiment(name, model_type, metrics, parameters=None, notes=\"\"):\n",
    "    \"\"\"Log experiment results\"\"\"\n",
    "    experiment = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'name': name,\n",
    "        'model_type': model_type,\n",
    "        'metrics': metrics,\n",
    "        'parameters': parameters or {},\n",
    "        'notes': notes\n",
    "    }\n",
    "    \n",
    "    experiment_log.append(experiment)\n",
    "    print(f\"✅ Experiment logged: {name}\")\n",
    "    return experiment\n",
    "\n",
    "def show_experiments():\n",
    "    \"\"\"Display experiment results\"\"\"\n",
    "    if not experiment_log:\n",
    "        print(\"No experiments logged yet\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(experiment_log)\n",
    "    \n",
    "    # Expand metrics into separate columns\n",
    "    metrics_df = pd.json_normalize(df['metrics'])\n",
    "    result_df = pd.concat([\n",
    "        df[['timestamp', 'name', 'model_type', 'notes']], \n",
    "        metrics_df\n",
    "    ], axis=1)\n",
    "    \n",
    "    print(\"📊 Experiment Results:\")\n",
    "    display(result_df)\n",
    "    return result_df\n",
    "\n",
    "# Example: Log the volatility model experiment\n",
    "log_experiment(\n",
    "    name=\"Volatility Prediction v1\",\n",
    "    model_type=\"RandomForestRegressor\",\n",
    "    metrics=vol_model.metrics,\n",
    "    parameters={'n_estimators': 100, 'random_state': 42},\n",
    "    notes=\"Initial baseline model with synthetic data\"\n",
    ")\n",
    "\n",
    "# Show all experiments\n",
    "show_experiments()\n",
    "\n",
    "print(\"\\n📋 Experiment tracking ready\")\n",
    "print(\"📝 Usage: log_experiment('name', 'model_type', metrics_dict, params_dict, 'notes')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deec18ec",
   "metadata": {},
   "source": [
    "## 🎯 Next Steps & Development Guidelines\n",
    "\n",
    "### 📚 Development Best Practices\n",
    "1. **Version Control**: Save notebook versions for important experiments\n",
    "2. **Documentation**: Add clear markdown explanations for complex analyses\n",
    "3. **Reproducibility**: Set random seeds and document dependencies\n",
    "4. **Testing**: Validate models on out-of-sample data\n",
    "\n",
    "### 🔄 Integration with Risk Platform\n",
    "- **API Integration**: Use RiskPlatformClient for data access\n",
    "- **Model Deployment**: Export models to shared directory for production use\n",
    "- **Monitoring**: Track model performance over time\n",
    "\n",
    "### 📊 Available Data Sources\n",
    "- **Risk API**: Portfolio and risk calculation data\n",
    "- **Market Data API**: Historical price and volume data\n",
    "- **Shared Storage**: `/home/jovyan/shared/data/`\n",
    "- **External APIs**: Connect to Bloomberg, Refinitiv, etc.\n",
    "\n",
    "### 🎯 Model Development Workflow\n",
    "1. **Data Exploration**: Use `explore_dataset()` function\n",
    "2. **Feature Engineering**: Create relevant risk factors\n",
    "3. **Model Training**: Use `RiskModelFramework` class\n",
    "4. **Validation**: Backtest and stress test models\n",
    "5. **Deployment**: Save to shared models directory\n",
    "6. **Monitoring**: Track performance in production\n",
    "\n",
    "### 📞 Support & Resources\n",
    "- **Documentation**: `/docs/` folder in repository\n",
    "- **Examples**: `/notebooks/examples/` for reference notebooks\n",
    "- **Team Support**: Contact data science team for assistance"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
